---
title: Spark读取csv文件方法
date: 2017-03-16 21:13:00
categories: 写点程序
tags: 
  - spark
  - csv
---

直接采用split的方法是不太好的，比如

```python
sc.textFile('file.csv')
    .map(lambda line: (line.split(',')[0], line.split(',')[1]))
    .collect()
```

这种方法容易出错，且读取的值无法自动识别类型。

可以稍微改进一下，比如

```python
import csv
rdd = sc.textFile("file.csv")
rdd = rdd.mapPartitions(lambda x: csv.reader(x))
```

或者，先将数据用pandas读入，然后再转化为DataFrame

```python
from pyspark import SparkContext
from pyspark.sql import SQLContext
import pandas as pd

sc = SparkContext('local','example')  # if using locally
sql_sc = SQLContext(sc)

Spark_Full = sc.emptyRDD()
chunk_100k = pd.read_csv("Your_Data_File.csv", chunksize=100000)
# if you have headers in your csv file:
headers = list(pd.read_csv("Your_Data_File.csv", nrows=0).columns)

for chunky in chunk_100k:
    Spark_Full +=  sc.parallelize(chunky.values.tolist())

YourSparkDataFrame = Spark_Full.toDF(headers)
# if you do not have headers, leave empty instead:
# YourSparkDataFrame = Spark_Full.toDF()
YourSparkDataFrame.show()
```

其实，Spark 2.0之后可以很方便使用SQLContext的读取csv文件，读取得到的格式为DataFrame：

```python
from pyspark import SparkContext
from pyspark.sql import SQLContext
sc = SparkContext()
sqlctx = SQLContext(sc)
sqlctx.read.csv(
    "some_input_file.csv", header=True, mode="DROPMALFORMED", schema=schema
)
```

或者

```python
(sqlctx.read
    .schema(schema)
    .option("header", "true")
    .option("mode", "DROPMALFORMED")
    .csv("some_input_file.csv"))
```

如果已知schema，则可以直接指定，防止自动推测发生错误

```python
from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import DoubleType, IntegerType, StringType

schema = StructType([
    StructField("A", IntegerType()),
    StructField("B", DoubleType()),
    StructField("C", StringType())
])
```

如果是Spark1.x，那么可以采用Databricks推出的[spark-csv](https://github.com/databricks/spark-csv)，其实这就是2.0版本以后读取csv的代码。

另外，github上还有另一个非官方版本的[spark-csv](https://github.com/seahboonsiew/pyspark-csv)。

